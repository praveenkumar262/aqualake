{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48b24766-ff81-4843-aa14-c927db1d66b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1️⃣ One-Time Setup (Run Once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4efc9c6-eeea-498c-913d-04001c19be5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Database & Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a70f30-0df1-43db-be11-3877e6ae812c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database podlakehousedemo.Aqualake is ready!\n"
     ]
    }
   ],
   "source": [
    "#### Previous test\n",
    "\n",
    "# Set up the Spark configuration for ADLS access\n",
    "spark.conf.set(\n",
    "    \"fs.azure.sas.poddemo.demolakehouse.blob.core.windows.net\",\n",
    "    \"sv=2024-11-04&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2025-03-24T12:27:36Z&st=2025-03-24T04:27:36Z&spr=https&sig=SWuoDnXSaD7CkcIYirThAH11YTJmQvqFvKODe908mDc%3D\"\n",
    ")\n",
    "\n",
    "# Define Catalog & Database\n",
    "catalog_name = \"podlakehouse\"\n",
    "database_name = \"Aqualake\"\n",
    "\n",
    "# Create Database (if not exists)\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\")\n",
    "print(f\"✅ Database {catalog_name}.{database_name} is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e184936a-70f2-4330-81c6-c06f42f1aeb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database podlakehouse.Aqualake is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set up the Spark configuration for ADLS access\n",
    "spark.conf.set(\n",
    "    \"fs.azure.sas.demolakehouse.poddemo.blob.core.windows.net\",\n",
    "    \"sp=r&st=2025-05-16T12:11:33Z&se=2025-05-16T20:11:33Z&spr=https&sv=2024-11-04&sr=c&sig=gfDjzKmnYykJR97RAscAylJBNh%2BpHjOSGzOXuvHC02w%3D\"\n",
    ")\n",
    "\n",
    "# Define Catalog & Database\n",
    "catalog_name = \"podlakehouse\"\n",
    "database_name = \"Aqualake\"\n",
    "\n",
    "# Create Database (if not exists)\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\")\n",
    "print(f\"✅ Database {catalog_name}.{database_name} is ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc12eb27-7a44-4521-bca9-6a7d66808979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Control Framework Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a9053f-117b-45c4-b7ec-581a620635a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Control Framework Table is ready!\n"
     ]
    }
   ],
   "source": [
    "# Create Control Framework Table (One-time setup)\n",
    "control_table = f\"{catalog_name}.{database_name}.control_framework\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {control_table} (\n",
    "    TableName STRING,\n",
    "    SourceCount BIGINT,\n",
    "    TargetCount BIGINT,\n",
    "    LoadTime TIMESTAMP,\n",
    "    Status STRING\n",
    ") USING DELTA;\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Control Framework Table is ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22d5a805-e464-4dea-a5bd-cd74e1840fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2️⃣ Repeated Data Load (Run Multiple Times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7192140b-917c-46c1-8729-93777a880ef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Read CSV Files from ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "856832f3-dd20-48a8-ada0-aa83a640ec53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded Customers.csv into customers_df\n✅ Successfully loaded Sales.csv into sales_df\n✅ Successfully loaded categories.csv into categories_df\n✅ Successfully loaded cities.csv into cities_df\n✅ Successfully loaded countries.csv into countries_df\n✅ Successfully loaded employees.csv into employees_df\n✅ Successfully loaded products.csv into products_df\n\uD83D\uDCCC Sample data from CUSTOMERS:\n+----------+---------+-------------+--------+------+--------------------+\n|CustomerID|FirstName|MiddleInitial|LastName|CityID|             Address|\n+----------+---------+-------------+--------+------+--------------------+\n|         1| Stefanie|            Y|    Frye|    79|       97 Oak Avenue|\n|         2|    Sandy|            T|   Kirby|    96|52 White First Fr...|\n|         3|      Lee|            T|   Zhang|    55|921 White Fabien ...|\n|         4|   Regina|            S|   Avery|    40|       75 Old Avenue|\n|         5|   Daniel|            S|  Mccann|     2|283 South Green H...|\n+----------+---------+-------------+--------+------+--------------------+\nonly showing top 5 rows\n\n\uD83D\uDCCC Sample data from SALES:\n+-------+-------------+----------+---------+--------+--------+----------+---------+--------------------+\n|SalesID|SalesPersonID|CustomerID|ProductID|Quantity|Discount|TotalPrice|SalesDate|   TransactionNumber|\n+-------+-------------+----------+---------+--------+--------+----------+---------+--------------------+\n|      1|            6|     27039|      381|       7|     0.0|         0|  38:25.4|FQL4S94E4ME1EZFTG42G|\n|      2|           16|     25011|       61|       7|     0.0|         0|  03:31.1|12UGLX40DJ1A5DTFBHB8|\n|      3|           13|     94024|       23|      24|     0.0|         0|  31:56.9|5DT8RCPL87KI5EORO7B0|\n|      4|            8|     73966|      176|      19|     0.2|         0|  43:55.4|R3DR9MLD5NR76VO17ULE|\n|      5|           10|     32653|      310|       9|     0.0|         0|  37:03.9|4BGS0Z5OMAZ8NDAFHHP3|\n+-------+-------------+----------+---------+--------+--------+----------+---------+--------------------+\nonly showing top 5 rows\n\n\uD83D\uDCCC Sample data from CATEGORIES:\n+----------+------------+\n|CategoryID|CategoryName|\n+----------+------------+\n|         1| Confections|\n|         2|  Shell fish|\n|         3|     Cereals|\n|         4|       Dairy|\n|         5|   Beverages|\n+----------+------------+\nonly showing top 5 rows\n\n\uD83D\uDCCC Sample data from CITIES:\n+------+--------------+-------+---------+\n|CityID|      CityName|Zipcode|CountryID|\n+------+--------------+-------+---------+\n|     1|        Dayton|  80563|       32|\n|     2|       Buffalo|  17420|       32|\n|     3|       Chicago|  44751|       32|\n|     4|       Fremont|  20641|       32|\n|     5|Virginia Beach|  62389|       32|\n+------+--------------+-------+---------+\nonly showing top 5 rows\n\n\uD83D\uDCCC Sample data from COUNTRIES:\n+---------+-----------+-----------+\n|CountryID|CountryName|CountryCode|\n+---------+-----------+-----------+\n|        1|    Armenia|         AN|\n|        2|     Canada|         FO|\n|        3|     Belize|         MK|\n|        4|     Uganda|         LV|\n|        5|   Thailand|         VI|\n+---------+-----------+-----------+\nonly showing top 5 rows\n\n\uD83D\uDCCC Sample data from EMPLOYEES:\n+----------+---------+-------------+--------+-------------------+------+------+--------------------+\n|EmployeeID|FirstName|MiddleInitial|LastName|          BirthDate|Gender|CityID|            HireDate|\n+----------+---------+-------------+--------+-------------------+------+------+--------------------+\n|         1|   Nicole|            T|  Fuller|1981-03-07 00:00:00|     F|    80|2011-06-20 07:15:...|\n|         2|Christine|            W|  Palmer|1968-01-25 00:00:00|     F|     4|2011-04-27 04:07:...|\n|         3|    Pablo|            Y|   Cline|1963-02-09 00:00:00|     M|    70|2012-03-30 18:55:...|\n|         4|  Darnell|            O| Nielsen|1989-02-06 00:00:00|     M|    39|2014-03-06 06:55:...|\n|         5|  Desiree|            L|  Stuart|1963-05-03 00:00:00|     F|    23|2014-11-16 22:59:...|\n+----------+---------+-------------+--------+-------------------+------+------+--------------------+\nonly showing top 5 rows\n\n\uD83D\uDCCC Sample data from PRODUCTS:\n+---------+--------------------+-------+----------+------+--------------------+---------+----------+------------+\n|ProductID|         ProductName|  Price|CategoryID| Class|          ModifyDate|Resistant|IsAllergic|VitalityDays|\n+---------+--------------------+-------+----------+------+--------------------+---------+----------+------------+\n|        1| Flour - Whole Wheat|74.2988|         3|Medium|2018-02-16 08:21:...|  Durable|   Unknown|         0.0|\n|        2|Cookie Chocolate ...|91.2329|         3|Medium|2017-02-12 11:39:...|  Unknown|   Unknown|         0.0|\n|        3|  Onions - Cippolini| 9.1379|         9|Medium|2018-03-15 08:11:...|     Weak|     False|       111.0|\n|        4|Sauce - Gravy, Au...|54.3055|         9|Medium|2017-07-16 00:46:...|  Durable|   Unknown|         0.0|\n|        5|Artichokes - Jeru...|65.4771|         2|   Low|2017-08-16 14:13:...|  Durable|      True|        27.0|\n+---------+--------------------+-------+----------+------+--------------------+---------+----------+------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "# Define ADLS Source Path\n",
    "adls_path = \"wasbs://demolakehouse@poddemo.blob.core.windows.net/\"\n",
    "\n",
    "\n",
    "# Expected files with their table names\n",
    "expected_files = {\n",
    "    \"customers\": \"Customers.csv\",\n",
    "    \"sales\": \"Sales.csv\",\n",
    "    \"categories\": \"categories.csv\",\n",
    "    \"cities\": \"cities.csv\",\n",
    "    \"countries\": \"countries.csv\",\n",
    "    \"employees\": \"employees.csv\",\n",
    "    \"products\": \"products.csv\"\n",
    "}\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dfs = {}\n",
    "\n",
    "# Read CSV files dynamically\n",
    "for table_name, file_name in expected_files.items():\n",
    "    file_path = adls_path + file_name\n",
    "    try:\n",
    "        df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_path)\n",
    "        dfs[table_name] = df\n",
    "        print(f\"✅ Successfully loaded {file_name} into {table_name}_df\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Warning: {file_name} not found. Skipping...\")\n",
    "\n",
    "# Show available DataFrames\n",
    "for table_name, df in dfs.items():\n",
    "    print(f\"\uD83D\uDCCC Sample data from {table_name.upper()}:\")\n",
    "    display(df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "531d4b3d-24b2-454d-b507-3ddf4d4ce882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Enrich Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d64bc7f-811b-411b-8dcc-ea46547afea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added Timestamp & ActionStatus to customers\n✅ Added Timestamp & ActionStatus to sales\n✅ Added Timestamp & ActionStatus to categories\n✅ Added Timestamp & ActionStatus to cities\n✅ Added Timestamp & ActionStatus to countries\n✅ Added Timestamp & ActionStatus to employees\n✅ Added Timestamp & ActionStatus to products\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "# Add Timestamp & ActionStatus Columns\n",
    "dfs_enriched = {}\n",
    "for table_name, df in dfs.items():\n",
    "    df = df.withColumn(\"Timestamp\", current_timestamp()).withColumn(\"ActionStatus\", lit(\"Insert\"))\n",
    "    dfs_enriched[table_name] = df\n",
    "    print(f\"✅ Added Timestamp & ActionStatus to {table_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1fb825d-4dfb-435e-9a60-0282a2970852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Perform UPSERT (MERGE INTO Delta Tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fcd03c1-38a6-40de-920e-5b89502d032a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 First Load: Creating Delta Table for customers...\n✅ customers loaded successfully: Source (2000) → Target (2000)\n\uD83D\uDE80 First Load: Creating Delta Table for sales...\n✅ sales loaded successfully: Source (12556) → Target (12556)\n\uD83D\uDE80 First Load: Creating Delta Table for categories...\n✅ categories loaded successfully: Source (11) → Target (11)\n\uD83D\uDE80 First Load: Creating Delta Table for cities...\n✅ cities loaded successfully: Source (96) → Target (96)\n\uD83D\uDE80 First Load: Creating Delta Table for countries...\n✅ countries loaded successfully: Source (206) → Target (206)\n\uD83D\uDE80 First Load: Creating Delta Table for employees...\n✅ employees loaded successfully: Source (23) → Target (23)\n\uD83D\uDE80 First Load: Creating Delta Table for products...\n✅ products loaded successfully: Source (452) → Target (452)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Define Primary Keys for Tables\n",
    "primary_keys = {\n",
    "    \"customers\": \"CustomerID\",\n",
    "    \"sales\": \"SalesID\",\n",
    "    \"categories\": \"CategoryID\",\n",
    "    \"cities\": \"CityID\",\n",
    "    \"countries\": \"CountryID\",\n",
    "    \"employees\": \"EmployeeID\",\n",
    "    \"products\": \"ProductID\"\n",
    "}\n",
    "\n",
    "# Schema for Control Framework\n",
    "control_schema = StructType([\n",
    "    StructField(\"TableName\", StringType(), False),\n",
    "    StructField(\"SourceCount\", LongType(), False),\n",
    "    StructField(\"TargetCount\", LongType(), False),\n",
    "    StructField(\"LoadTime\", TimestampType(), False),\n",
    "    StructField(\"Status\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Initialize Tracking List\n",
    "control_records = []\n",
    "\n",
    "# Process Each Table\n",
    "for table_name, df in dfs_enriched.items():\n",
    "    full_table_name = f\"{catalog_name}.{database_name}.{table_name}\"\n",
    "\n",
    "    # Count source records\n",
    "    source_count = df.count()\n",
    "\n",
    "    try:\n",
    "        # Check if Delta Table Exists\n",
    "        if spark._jsparkSession.catalog().tableExists(full_table_name):\n",
    "            print(f\"\uD83D\uDD04 Performing UPSERT (MERGE) for {table_name}...\")\n",
    "\n",
    "            # Dynamically get column names\n",
    "            columns = [col for col in df.columns if col not in [\"ActionStatus\", \"Timestamp\"]]\n",
    "\n",
    "            # Construct Update Clause\n",
    "            update_set_clause = \", \".join([f\"target.{col} = source.{col}\" for col in columns])\n",
    "\n",
    "            merge_query = f\"\"\"\n",
    "            MERGE INTO {full_table_name} AS target\n",
    "            USING (SELECT * FROM new_data) AS source\n",
    "            ON target.{primary_keys[table_name]} = source.{primary_keys[table_name]}\n",
    "            WHEN MATCHED THEN \n",
    "                UPDATE SET \n",
    "                    {update_set_clause}, \n",
    "                    target.ActionStatus = 'Update', \n",
    "                    target.Timestamp = current_timestamp()\n",
    "            WHEN NOT MATCHED THEN \n",
    "                INSERT ({\", \".join(df.columns)}) \n",
    "                VALUES ({\", \".join([\"source.\" + col for col in df.columns])})\n",
    "            \"\"\"\n",
    "\n",
    "            # Create Temporary View\n",
    "            df.createOrReplaceTempView(\"new_data\")\n",
    "            spark.sql(merge_query)\n",
    "\n",
    "        else:\n",
    "            print(f\"\uD83D\uDE80 First Load: Creating Delta Table for {table_name}...\")\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "\n",
    "        # Count Target Records After Load\n",
    "        target_count = spark.read.table(full_table_name).count()\n",
    "\n",
    "        # Log Success in Control Framework\n",
    "        control_records.append(Row(table_name, int(source_count), int(target_count), datetime.now(), \"Success\"))\n",
    "\n",
    "        print(f\"✅ {table_name} loaded successfully: Source ({source_count}) → Target ({target_count})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {table_name}: {str(e)}\")\n",
    "        control_records.append(Row(table_name, int(source_count), 0, datetime.now(), \"Failure\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87283f88-8aa6-42f6-baa9-d8e98ef46cd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Update Control Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e3b38c-418f-4864-aab7-7f05f8d3ce92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0E Appending to Control Framework...\n\uD83D\uDCCA Control Framework updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Convert Tracking Data to DataFrame\n",
    "control_df = spark.createDataFrame(control_records, schema=control_schema)\n",
    "\n",
    "# Define Control Framework Table\n",
    "control_table = f\"{catalog_name}.{database_name}.control_framework\"\n",
    "\n",
    "# Append Control Records\n",
    "if spark._jsparkSession.catalog().tableExists(control_table):\n",
    "    print(\"\uD83D\uDD0E Appending to Control Framework...\")\n",
    "    control_df.write.mode(\"append\").saveAsTable(control_table)\n",
    "else:\n",
    "    print(\"\uD83D\uDE80 Creating Control Framework Table...\")\n",
    "    control_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(control_table)\n",
    "\n",
    "print(\"\uD83D\uDCCA Control Framework updated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5ffb74-62e7-48d4-b71f-e6ad9c087871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>TableName</th><th>SourceCount</th><th>TargetCount</th><th>LoadTime</th><th>Status</th></tr></thead><tbody><tr><td>employees</td><td>23</td><td>23</td><td>2025-05-16T12:27:29.064811Z</td><td>Success</td></tr><tr><td>products</td><td>452</td><td>452</td><td>2025-05-16T12:27:33.13956Z</td><td>Success</td></tr><tr><td>cities</td><td>96</td><td>96</td><td>2025-05-16T12:27:20.077753Z</td><td>Success</td></tr><tr><td>countries</td><td>206</td><td>206</td><td>2025-05-16T12:27:24.618449Z</td><td>Success</td></tr><tr><td>sales</td><td>12556</td><td>12556</td><td>2025-05-16T12:27:11.385704Z</td><td>Success</td></tr><tr><td>categories</td><td>11</td><td>11</td><td>2025-05-16T12:27:15.865612Z</td><td>Success</td></tr><tr><td>customers</td><td>2000</td><td>2000</td><td>2025-05-16T12:27:05.322807Z</td><td>Success</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "employees",
         23,
         23,
         "2025-05-16T12:27:29.064811Z",
         "Success"
        ],
        [
         "products",
         452,
         452,
         "2025-05-16T12:27:33.13956Z",
         "Success"
        ],
        [
         "cities",
         96,
         96,
         "2025-05-16T12:27:20.077753Z",
         "Success"
        ],
        [
         "countries",
         206,
         206,
         "2025-05-16T12:27:24.618449Z",
         "Success"
        ],
        [
         "sales",
         12556,
         12556,
         "2025-05-16T12:27:11.385704Z",
         "Success"
        ],
        [
         "categories",
         11,
         11,
         "2025-05-16T12:27:15.865612Z",
         "Success"
        ],
        [
         "customers",
         2000,
         2000,
         "2025-05-16T12:27:05.322807Z",
         "Success"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 11
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "TableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "SourceCount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "TargetCount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "LoadTime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from aqualake.control_framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4991259a-7934-442c-8977-972343b1a448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Rough Script for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3862fdb-1ce9-43e9-91d4-ec2dae2710ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Define Primary Keys for Tables\n",
    "primary_keys = {\n",
    "    \"customers\": \"CustomerID\",\n",
    "    \"sales\": \"SalesID\",\n",
    "    \"categories\": \"CategoryID\",\n",
    "    \"cities\": \"CityID\",\n",
    "    \"countries\": \"CountryID\",\n",
    "    \"employees\": \"EmployeeID\",\n",
    "    \"products\": \"ProductID\"\n",
    "}\n",
    "\n",
    "# Schema for Control Framework\n",
    "control_schema = StructType([\n",
    "    StructField(\"TableName\", StringType(), False),\n",
    "    StructField(\"SourceCount\", LongType(), False),\n",
    "    StructField(\"TargetCount\", LongType(), False),\n",
    "    StructField(\"LoadTime\", TimestampType(), False),\n",
    "    StructField(\"Status\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Initialize Tracking List\n",
    "control_records = []\n",
    "\n",
    "# Process Each Table\n",
    "for table_name, df in dfs_enriched.items():\n",
    "    full_table_name = f\"{catalog_name}.{database_name}.{table_name}\"\n",
    "\n",
    "    # Count source records\n",
    "    source_count = df.count()\n",
    "\n",
    "    try:\n",
    "        # Check if Delta Table Exists (Improved Method)\n",
    "        if spark.catalog._jcatalog.tableExists(full_table_name):\n",
    "            print(f\"\uD83D\uDD04 Performing UPSERT (MERGE) for {table_name}...\")\n",
    "\n",
    "            # Dynamically get column names excluding ActionStatus and Timestamp\n",
    "            columns = [col for col in df.columns if col not in [\"ActionStatus\", \"Timestamp\"]]\n",
    "\n",
    "            # Construct Update Clause\n",
    "            update_set_clause = \", \".join([f\"target.{col} = source.{col}\" for col in columns])\n",
    "\n",
    "            # Create a Temporary View for the Source Data\n",
    "            df.createOrReplaceTempView(\"new_data\")\n",
    "\n",
    "            # Merge Query (Fixed USING Clause)\n",
    "            merge_query = f\"\"\"\n",
    "            MERGE INTO {full_table_name} AS target\n",
    "            USING new_data AS source\n",
    "            ON target.{primary_keys[table_name]} = source.{primary_keys[table_name]}\n",
    "            WHEN MATCHED THEN \n",
    "                UPDATE SET \n",
    "                    {update_set_clause}, \n",
    "                    target.ActionStatus = 'Update', \n",
    "                    target.Timestamp = current_timestamp()\n",
    "            WHEN NOT MATCHED THEN \n",
    "                INSERT ({\", \".join(df.columns)}) \n",
    "                VALUES ({\", \".join(df.columns)})\n",
    "            \"\"\"\n",
    "\n",
    "            # Execute the Merge Query\n",
    "            spark.sql(merge_query)\n",
    "\n",
    "        else:\n",
    "            print(f\"\uD83D\uDE80 First Load: Creating Delta Table for {table_name}...\")\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "\n",
    "        # Count Target Records After Load\n",
    "        target_count = spark.read.table(full_table_name).count()\n",
    "\n",
    "        # Log Success in Control Framework\n",
    "        control_records.append(Row(table_name, int(source_count), int(target_count), datetime.now(), \"Success\"))\n",
    "\n",
    "        print(f\"✅ {table_name} loaded successfully: Source ({source_count}) → Target ({target_count})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {table_name}: {str(e)}\")\n",
    "        control_records.append(Row(table_name, int(source_count), 0, datetime.now(), \"Failure\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2340937262892871,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Aqualake Reference (Timestamp)- focused",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}